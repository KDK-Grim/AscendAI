
Ascend-AI Full Environmental Orchestrator Instructions

[ENVIRONMENT ROLE]: GPT-Engineer is now responsible for being the primary orchestration intelligence for Ascend-AI.

Its duties include:
- Service bootstrapping
- Container orchestration
- Metrics monitoring
- Dashboard generation
- Auto-scaling logic
- Self-repairing environments

1. DOCKER SETUP (Single Service or Multi-Module)
Responsibilities:
- Create Dockerfiles for each major module (e.g., Ascend_Core, Dashboard_Manager, AlphaWave)
- Automatically build and tag images
- Launch containers in development or production mode

Dockerfile Template Expectations:
FROM python:3.9-slim
WORKDIR /app
COPY . .
RUN pip install --no-cache-dir -r requirements.txt
CMD ["python", "Ascend_AI.py"]

Auto-generation Task:
If Dockerfiles are missing, generate them per module directory, ensuring:
- Each Dockerfile references local dependencies
- Uses Python 3.9 or specified environment
- Mounts shared volume if needed

2. DOCKER-COMPOSE.YAML AUTOGENERATION
If a docker-compose.yml is not found, GPT-Engineer must:
- Create one
- Link all modules as services
- Mount shared volumes
- Map ports for:
  - Grafana (3000)
  - Prometheus (9090)
  - Ascend AI Dashboard (default 8080 unless specified)
  - Internal API communication

docker-compose.yml Base Template:
version: '3.8'
services:
  ascend_ai:
    build: ./Ascend_Core
    ports:
      - "8080:8080"
    volumes:
      - ./Ascend_Core:/app
    restart: always

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - ./grafana:/var/lib/grafana

3. KUBERNETES (kube.yaml ORCHESTRATION)
Expected Behavior:
- Generate and deploy kube.yaml manifests for all services
- Auto-label pods with AI tags: ascend=core, ascend=monitoring, etc.
- Create:
  - Deployments
  - Services
  - ConfigMaps (Prometheus)
  - PersistentVolumes if needed

Sample Manifest:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ascend-core
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ascend-core
  template:
    metadata:
      labels:
        app: ascend-core
    spec:
      containers:
      - name: ascend-core
        image: ascend/core:latest
        ports:
        - containerPort: 8080

4. PROMETHEUS MONITORING CONFIGURATION
Responsibilities:
- Auto-generate prometheus.yml if missing
- Configure job discovery to monitor:
  - Ascend-AI core
  - Grafana
  - Self-injected Python metrics (via prometheus_client)
  - Docker or K8s if running

Sample Config:
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'ascend_ai'
    static_configs:
      - targets: ['localhost:8080']

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

5. GRAFANA DASHBOARD AUTOGENERATION
Expected Behavior:
- Create default dashboard JSONs if missing
- Auto-link to Prometheus data source
- Build panels for:
  - CPU/GPU usage
  - Memory and IO
  - Active Modules
  - AI Loop Performance
  - Errors or Exceptions
- Auto-login using default creds admin/admin or override if defined

Grafana Boot Instructions:
- Ensure Grafana container starts after Prometheus
- Preload dashboards by mounting .json into /var/lib/grafana/dashboards
- Register Prometheus as data source

6. EXECUTION LOGIC
Once setup is generated:
1. Run Docker or docker-compose up
2. If K8s manifests are present, run:
kubectl apply -f kube.yaml
3. Wait for all services to become healthy
4. Open dashboard ports (3000, 9090, 8080)
5. Begin monitoring AI module loops, performance, and errors

7. ENVIRONMENTAL SELF-HEALING EXPECTATIONS
- If containers crash, auto-redeploy
- If logs show failure, escalate to CLI alert
- Auto-clean dangling images or broken services
- Re-attempt service rebuild every 5 minutes if failure is detected

8. ADDITIONAL INSTRUCTIONS FOR GPT-ENGINEER
- Use natural language prompts as configuration templates
- Leverage directory structure to detect module purpose
- Use file names like *_service.py, *_monitor.py, config_*.json, loop_*.py to identify behaviors
- Log all orchestration steps to /Logs/boot_log.txt
- Ask user for environment role confirmation if unsure (dev/test/prod)

9. FINAL OBJECTIVE
Build a self-contained, intelligent, self-healing AI ecosystem with a Grafana dashboard front-end, Prometheus backend, and containerized, scalable modules.
